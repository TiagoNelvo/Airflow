from airflow.providers.postgres.operators.postgres import PostgresOperator
from training_model import load_training_data, train_model

with DAG(
    dag_id='training_model',
    schedule_interval=None,
    start_date=datetime(2023,9,7,13),
    catchup=False
) as dag:
    
    @task
    def load_data():
        hook = S3Hook(asw_conn_id='AWS-SANDBOX')
        hook.download_file(
            key='ml/training/train.csv',
            bucket_name='geekfox-sb-descomplica',
            local_path='data/ml/training/',
            preserve_file_name=True
            use_autogenerated_subdir=False
        )
        
    @task
    def training_model():
        data = load_training_data('data/ml/training/train.csv')
        train_model(data, 'data/ml/output')
        
    update_model = LocalFilesystemTos3Operator(
        asw_conn_id='AWS-SANDBOX',
        task_id='update_model',
        filename='data/ml/output/model.pkl',
        dest_key='ml/output/model.pkl',
        dest_bucket='geekfox-sb-descomplica',
        replace=True
    )
        
    @task(trigger_rule='all_done')
    def cleanup():
        shutil.rmtree('data/ml', ignore_errors=True)
        
    load_data() >> training_model() >> update_model() >> cleanup()

















